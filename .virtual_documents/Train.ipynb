





# Parameters
SEQ_LEN = 24  # past 24 hours for each sample
TARGET_COL = 'OT'  # target variable: Oil Temperature
EPOCHS = 30 
BATCH_SIZE = 32


import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
get_ipython().getoutput("pip install vmdpy")



save_dir = os.path.expanduser("~/Project/AttnLstm/data/raw")
os.makedirs(save_dir, exist_ok=True)
# Full file path
file_path = os.path.join(save_dir, "ETTh1.csv")
# Load it whenever needed
df = pd.read_csv(file_path)
print(df.head())
df.columns = df.columns.str.strip().str.replace('\ufeff', '')
print(df.columns)


# Ensure datetime type
df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)

# Select target
target = 'OT'
data = df[[target]].values

# Visualize
plt.figure(figsize=(12, 4))
plt.plot(df.index, df['OT'], label='Oil Temperature (OT)')
plt.title('ETTh1 - Oil Temperature Time Series')
plt.xlabel('Date')
plt.ylabel('OT')
plt.legend()
plt.show()


# Normalize target
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(data)

plt.figure(figsize=(12, 4))
plt.plot(df.index, data_scaled)
plt.title("Normalized Target (OT) over Time")
plt.xlabel("Date")
plt.ylabel("Normalized OT")
plt.show()





from vmdpy import VMD

# VMD parameters
alpha = 2000       # moderate bandwidth constraint
tau = 0.            # noise-tolerance (0 for no strict fidelity)
K = 4               # number of modes to extract
DC = 0              # no DC part imposed
init = 1            # initialize omegas uniformly
tol = 1e-7

# Apply VMD
u, u_hat, omega = VMD(data_scaled.flatten(), alpha, tau, K, DC, init, tol)

plt.figure(figsize=(12, 6))
for i in range(K):
    plt.subplot(K, 1, i+1)
    plt.plot(u[i])
    plt.title(f"IMF {i+1}")
plt.tight_layout()
plt.show()






# Parameters
SEQ_LEN = 24
TARGET_COL = 'OT'

# Select relevant columns
df_model = df[['OT', 'HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL']].copy()

# Split index (80% train, 20% test)
split_idx = int(len(df_model) * 0.8)

# Split data chronologically
train_data = df_model[:split_idx]
test_data  = df_model[split_idx:]

# Fit scaler only on training data
scaler = MinMaxScaler()
train_scaled = scaler.fit_transform(train_data)
test_scaled  = scaler.transform(test_data)

# Sequence creation function
def create_sequences(data, seq_len):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i + seq_len])
        y.append(data[i + seq_len, 0])  # target: OT
    return np.array(X), np.array(y)

# Create sequences
X_train, y_train = create_sequences(train_scaled, SEQ_LEN)
X_test, y_test   = create_sequences(test_scaled, SEQ_LEN)

# If using pure LSTM on OT only
X_train_lstm = X_train[:, :, 0:1]
X_test_lstm  = X_test[:, :, 0:1]

print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")






from tensorflow.keras.layers import Input, LSTM, Dense, Attention, Concatenate, Lambda
from tensorflow.keras.models import Model
import tensorflow as tf

inputs = Input(shape=(SEQ_LEN, 1))
x =  LSTM(64, return_sequences=True)(inputs)
x =  LSTM(64, return_sequences=True)(x)
x =  LSTM(64, return_sequences=True)(x)
x =  LSTM(128, return_sequences=True)(x)
lstm_out = LSTM(64, return_sequences=True)(x)

# Attention
context = Attention()([lstm_out, lstm_out])

# Extract last timestep safely
last_output = Lambda(lambda x: x[:, -1, :])(lstm_out)

# Mean over context dimension
context_mean = Lambda(lambda x: tf.reduce_mean(x, axis=1))(context)

# Combine last output + context
concat = Concatenate()([last_output, context_mean])
dense_out = Dense(1)(concat)

# Define model
model = Model(inputs, dense_out)
model.compile(optimizer='adam', loss='mse')
model.summary()

# Train the model
history = model.fit(
    X_train_lstm, y_train,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_data=(X_test_lstm, y_test),
    verbose=1
)

plt.figure(figsize=(8, 4))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.title('Attention-LSTM Training Performance')
plt.legend()
plt.grid(True)
plt.show()

# Make predictions
y_pred = model.predict(X_test_lstm)

# Plot predictions vs true values
plt.figure(figsize=(12, 5))
plt.plot(y_test, label='True OT', color='blue')
plt.plot(y_pred, label='Predicted OT', color='red')
plt.xlabel('Time Steps')
plt.ylabel('Oil Temperature')
plt.title('Attention-LSTM Predictions vs True Values')
plt.legend()
plt.grid(True)
plt.show()





from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping

# Advanced LSTM model
advanced_lstm = Sequential([
    Bidirectional(LSTM(64, return_sequences=True), input_shape=(SEQ_LEN, 1)),
    Dropout(0.2),
    LSTM(128, return_sequences=True),
    LSTM(64, return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1)
])

advanced_lstm.compile(optimizer='adam', loss='mse')
advanced_lstm.summary()

# Early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train model
history_adv = advanced_lstm.fit(
    X_train_lstm, y_train,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_data=(X_test_lstm, y_test),
    callbacks=[early_stop],
    verbose=1
)

# Plot training history
plt.figure(figsize=(8, 4))
plt.plot(history_adv.history['loss'], label='Training Loss')
plt.plot(history_adv.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.title('Advanced LSTM Training Performance')
plt.legend()
plt.grid(True)
plt.show()

# Make predictions
y_pred_adv = advanced_lstm.predict(X_test_lstm)

# Plot predictions vs true values
plt.figure(figsize=(12, 5))
plt.plot(y_test, label='True OT', color='blue')
plt.plot(y_pred_adv, label='Predicted OT', color='green')
plt.xlabel('Time Steps')
plt.ylabel('Oil Temperature')
plt.title('Advanced LSTM Predictions vs True Values')
plt.legend()
plt.grid(True)
plt.show()

