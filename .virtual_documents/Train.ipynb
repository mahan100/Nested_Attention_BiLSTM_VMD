





# Parameters
SEQ_LEN = 24  # past 24 hours for each sample
TARGET_COL = 'OT'  # target variable: Oil Temperature
EPOCHS = 30 
BATCH_SIZE = 32

# VMD parameters
DC = 0              # no DC part imposed
init = 1            # initialize omegas uniformly
tol = 1e-7
K = 4                    # number of VMD modes (tuneable)
alpha = 2000             # VMD alpha (tuneable)
tau = 0.0                # VMD tau

LSTM_UNITS = 128
DROPOUT = 0.2
LEARNING_RATE = 1e-3


import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import os
from tensorflow.keras.layers import MultiHeadAttention
from tensorflow.keras.layers import Softmax
from vmdpy import VMD
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.layers import RepeatVector


save_dir = os.path.expanduser("~/Project/AttnLstm/data/raw")
os.makedirs(save_dir, exist_ok=True)
# Full file path
file_path = os.path.join(save_dir, "ETTh1.csv")
# Load it whenever needed
df = pd.read_csv(file_path)
print(df.head())
df.columns = df.columns.str.strip().str.replace('\ufeff', '')
print(df.columns)
# Ensure datetime type
df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)
data = df[[TARGET_COL]].values



# Visualize
plt.figure(figsize=(12, 4))
plt.plot(df.index, df['OT'], label='Oil Temperature (OT)')
plt.title('ETTh1 - Oil Temperature Time Series')
plt.xlabel('Date')
plt.ylabel('OT')
plt.legend()
plt.show()





# =====================================================
# Split Train/Test
# =====================================================
split_idx = int(len(df) * 0.8)
train_data = df[:split_idx]
test_data  = df[split_idx:]
# =====================================================
# Helper for VMD-based training data
# =====================================================
def create_sequences(X, y, seq_len):
    Xs, ys = [], []
    limit = min(len(X), len(y)) - seq_len
    for i in range(limit):
        Xs.append(X[i:i+seq_len])
        ys.append(y[i+seq_len])
    return np.array(Xs), np.array(ys)

    
def train_test(alpha=1, tau=1, K=1):
    # --- VMD on train and test ---
    u_train, _, _ = VMD(train_data[TARGET_COL].values, alpha, tau, K, DC=0, init=1, tol=1e-7)
    u_test,  _, _ = VMD(test_data[TARGET_COL].values,  alpha, tau, K, DC=0, init=1, tol=1e-7)

    vmd_train = np.stack(u_train, axis=1)
    vmd_test  = np.stack(u_test,  axis=1)

    # --- Scale features & target ---
    scaler_x = MinMaxScaler()
    scaler_y = MinMaxScaler()
    train_scaled_vmd = scaler_x.fit_transform(vmd_train)
    train_scaled_target = scaler_y.fit_transform(train_data[[TARGET_COL]])
    test_scaled_vmd  = scaler_x.transform(vmd_test)
    test_scaled_target  = scaler_y.transform(test_data[[TARGET_COL]])

    # --- Sequence generation (aligned) ---
    X_train_vmd, y_train_vmd = create_sequences(train_scaled_vmd, train_scaled_target, SEQ_LEN)
    X_test_vmd,  y_test_vmd  = create_sequences(test_scaled_vmd,  test_scaled_target,  SEQ_LEN)
    X_train_simple, y_train_simple = create_sequences(train_scaled_target,train_scaled_target,SEQ_LEN)
    X_test_simple, y_test_simple = create_sequences(test_scaled_target,test_scaled_target,SEQ_LEN)
    y_test_simple_true = scaler_y.inverse_transform(y_test_simple.reshape(-1, 1)).flatten()
    y_test_vmd_true = scaler_y.inverse_transform(y_test_vmd)


    # ‚úÖ Align lengths with simple test (optional for comparison)
    # min_len = min(len(y_test_vmd_seq), len(y_test_simple_seq))
    # y_test_vmd_seq = y_test_vmd_seq[-min_len:]
    # y_test_simple_seq = y_test_simple_seq[-min_len:]
    # X_test_vmd_seq = X_test_vmd_seq[-min_len:]

    return y_test_vmd_true,y_test_simple_true,scaler_y,X_train_vmd, y_train_vmd, X_test_vmd, y_test_vmd,X_train_simple,y_train_simple,X_test_simple,y_test_simple












import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Attention, Concatenate
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from vmdpy import VMD
import random

# =====================================================
# 1Ô∏è‚É£ Global Config
# =====================================================
SEQ_LEN = SEQ_LEN        # sequence length
EPOCHS_OPT = 4           # epochs during GA fitness
EPOCHS_FINAL = EPOCHS    # final retraining

# =====================================================
# 2Ô∏è‚É£ ATT-LSTM Model Builder
# =====================================================
def build_attlstm_model(input_shape, lstm1_units=128, lstm2_units=64, dense_units=32):
    inp = Input(shape=input_shape)
    lstm_out = LSTM(lstm1_units, return_sequences=True)(inp)
    lstm_out = Dropout(0.2)(lstm_out)

    # Attention
    query = Dense(lstm1_units)(lstm_out)
    value = Dense(lstm1_units)(lstm_out)
    attention_out = Attention()([query, value])
    attention_out = Concatenate()([lstm_out, attention_out])

    # Decoder
    lstm_dec = LSTM(lstm2_units, return_sequences=False)(attention_out)
    dense_out = Dense(dense_units, activation='relu')(lstm_dec)
    final_out = Dense(1)(dense_out)

    model = Model(inputs=inp, outputs=final_out)
    model.compile(optimizer=Adam(1e-3), loss='mse', metrics=['mae'])
    return model

# =====================================================
# 4Ô∏è‚É£ Fitness Function (GA objective)
# =====================================================
def fitness(params, data):
    alpha, tau, K = params['alpha'], params['tau'], params['K']
    lstm1_units, lstm2_units, dense_units = (
        params['lstm1_units'], params['lstm2_units'], params['dense_units']
    )
    
    y_test_vmd_true,_,_,X_train_vmd, y_train_vmd, X_test_vmd, y_test_vmd,_,_,_,_= train_test(alpha, tau, K)
    
    # --- Model ---
    model = build_attlstm_model(
        (SEQ_LEN, K),
        lstm1_units=lstm1_units,
        lstm2_units=lstm2_units,
        dense_units=dense_units
    )

    history = model.fit(
        X_train_vmd, y_train_vmd,
        validation_data=(X_test_vmd, y_test_vmd),
        epochs=EPOCHS_OPT,
        batch_size=BATCH_SIZE,
        verbose=0
    )

    mse = min(history.history['val_loss'])
    return mse

# =====================================================
# 5Ô∏è‚É£ Genetic Algorithm (GA)
# =====================================================
def random_params():
    return {
        'alpha': random.uniform(500, 3000),
        'tau': random.uniform(0.0, 0.5),
        'K': random.randint(3, 8),
        'lstm1_units': random.choice([64, 96, 128, 160]),
        'lstm2_units': random.choice([32, 48, 64, 96]),
        'dense_units': random.choice([16, 32, 48, 64])
    }

def mutate(params):
    new_params = params.copy()
    if random.random() < 0.4:
        new_params['alpha'] *= random.uniform(0.8, 1.2)
    if random.random() < 0.4:
        new_params['tau'] += random.uniform(-0.05, 0.05)
    if random.random() < 0.3:
        new_params['K'] = max(2, min(10, new_params['K'] + random.choice([-1, 1])))
    if random.random() < 0.4:
        new_params['lstm1_units'] = random.choice([64, 96, 128, 160])
    if random.random() < 0.4:
        new_params['lstm2_units'] = random.choice([32, 48, 64, 96])
    if random.random() < 0.4:
        new_params['dense_units'] = random.choice([16, 32, 48, 64])
    return new_params

def evolve_population(data, pop_size=4, generations=3):
    population = [random_params() for _ in range(pop_size)]
    for g in range(generations):
        print(f"\nüß¨ Generation {g+1}/{generations}")
        scores = []
        for i, params in enumerate(population):
            mse = fitness(params, data)
            scores.append((mse, params))
            print(f"  Candidate {i+1}: MSE = {mse:.4f}, params = {params}")
        scores.sort(key=lambda x: x[0])
        best_params = scores[0][1]
        print(f"  ‚úÖ Best so far: {best_params}")
        # Evolve
        new_pop = [scores[0][1], scores[1][1]]
        while len(new_pop) < pop_size:
            parent = random.choice(new_pop)
            new_pop.append(mutate(parent))
        population = new_pop
    return best_params

# =====================================================
# 6Ô∏è‚É£ Run Optimization
# =====================================================
best_params = evolve_population(df, pop_size=4, generations=3)
print("\nüèÅ Best parameters found:", best_params)
# =====================================================
# 7Ô∏è‚É£ Retrain with Best Params (no leakage)
# =====================================================
alpha, tau, K = best_params['alpha'], best_params['tau'], best_params['K']
lstm1_units, lstm2_units, dense_units = (
    best_params['lstm1_units'], best_params['lstm2_units'], best_params['dense_units']
)

y_test_vmd_true,_,scaler_y,X_train_vmd, y_train_vmd, X_test_vmd, y_test_vmd,_,_,_,_= train_test(alpha, tau, K) 


model = build_attlstm_model((SEQ_LEN, K), lstm1_units, lstm2_units, dense_units)
model.summary()

history_attlstm_vmd_ga = model.fit(
    X_train_vmd, y_train_vmd,
    validation_data=(X_test_vmd, y_test_vmd),
    epochs=EPOCHS_FINAL,
    batch_size=BATCH_SIZE,
    verbose=1
)

# Predict (inverse-transform from original OT scaling)
y_pred_attlstm_vmd_ga_scaled = model.predict(X_test_vmd)
y_pred_attlstm_vmd_ga_true = scaler_y.inverse_transform(y_pred_attlstm_vmd_ga_scaled)

# =====================================================
# 8Ô∏è‚É£ Visualization
# =====================================================
plt.figure(figsize=(8,4))
plt.plot(history_attlstm_vmd_ga.history['loss'], label='Train Loss')
plt.plot(history_attlstm_vmd_ga.history['val_loss'], label='Val Loss')
plt.title('Training & Validation Loss (ATT-LSTM + VMD + GA)')
plt.legend(); plt.grid(True); plt.show()

plt.figure(figsize=(12,5))
plt.plot(y_test_vmd_true, label='True OT', color='black', linewidth=1)
plt.plot(y_pred_attlstm_vmd_ga_true, label='Predicted OT', color='red', alpha=0.8)
plt.title('True vs Predicted OT (Optimized ATT-LSTM + VMD + GA)')
plt.xlabel('Time Steps'); plt.ylabel('OT Value')
plt.legend(); plt.grid(True); plt.show()

plt.figure(figsize=(10,4))
residuals = y_test_vmd_true - y_pred_attlstm_vmd_ga_true
plt.plot(residuals, color='purple')
plt.title('Residuals (True - Predicted)')
plt.grid(True); plt.show()





import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Attention, Concatenate
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from vmdpy import VMD
import random

# =====================================================
# 1Ô∏è‚É£ Global Config
# =====================================================
SEQ_LEN = SEQ_LEN        # sequence length
EPOCHS_OPT = 4           # epochs during GA fitness
EPOCHS_FINAL = EPOCHS    # final retraining

# =====================================================
# 2Ô∏è‚É£ ATT-LSTM Model Builder
# =====================================================
def build_attlstm_model(input_shape, lstm1_units=128, lstm2_units=64, dense_units=32):
    inp = Input(shape=input_shape)
    lstm_out = LSTM(lstm1_units, return_sequences=True)(inp)
    lstm_out = Dropout(0.2)(lstm_out)

    # Attention
    query = Dense(lstm1_units)(lstm_out)
    value = Dense(lstm1_units)(lstm_out)
    attention_out = Attention()([query, value])
    attention_out = Concatenate()([lstm_out, attention_out])

    # Decoder
    lstm_dec = LSTM(lstm2_units, return_sequences=False)(attention_out)
    dense_out = Dense(dense_units, activation='relu')(lstm_dec)
    final_out = Dense(1)(dense_out)

    model = Model(inputs=inp, outputs=final_out)
    model.compile(optimizer=Adam(1e-3), loss='mse', metrics=['mae'])
    return model

# =====================================================
# 4Ô∏è‚É£ Fitness Function (GA objective)
# =====================================================
def fitness(params, data):
    lstm1_units, lstm2_units, dense_units = (
        params['lstm1_units'], params['lstm2_units'], params['dense_units']
    )
    
    _,y_test_simple_true,scaler_y,_,_,_,_,X_train_simple,y_train_simple,X_test_simple,y_test_simple = train_test() 
   
    # --- Model ---
    k = 1
    model = build_attlstm_model(
        (SEQ_LEN, K),
        lstm1_units=lstm1_units,
        lstm2_units=lstm2_units,
        dense_units=dense_units
    )

    history = model.fit(
        X_train_simple, y_train_simple,
        validation_data=(X_test_simple, y_test_simple),
        epochs=EPOCHS_OPT,
        batch_size=BATCH_SIZE,
        verbose=0
    )

    mse = min(history.history['val_loss'])
    return mse

# =====================================================
# 5Ô∏è‚É£ Genetic Algorithm (GA)
# =====================================================
def random_params():
    return {
        
        'lstm1_units': random.choice([64, 96, 128, 160]),
        'lstm2_units': random.choice([32, 48, 64, 96]),
        'dense_units': random.choice([16, 32, 48, 64])
    }

def mutate(params):
    new_params = params.copy()
    if random.random() < 0.4:
        new_params['lstm1_units'] = random.choice([64, 96, 128, 160])
    if random.random() < 0.4:
        new_params['lstm2_units'] = random.choice([32, 48, 64, 96])
    if random.random() < 0.4:
        new_params['dense_units'] = random.choice([16, 32, 48, 64])
    return new_params

def evolve_population(data, pop_size=4, generations=3):
    population = [random_params() for _ in range(pop_size)]
    for g in range(generations):
        print(f"\nüß¨ Generation {g+1}/{generations}")
        scores = []
        for i, params in enumerate(population):
            mse = fitness(params, data)
            scores.append((mse, params))
            print(f"  Candidate {i+1}: MSE = {mse:.4f}, params = {params}")
        scores.sort(key=lambda x: x[0])
        best_params = scores[0][1]
        print(f"  ‚úÖ Best so far: {best_params}")
        # Evolve
        new_pop = [scores[0][1], scores[1][1]]
        while len(new_pop) < pop_size:
            parent = random.choice(new_pop)
            new_pop.append(mutate(parent))
        population = new_pop
    return best_params

# =====================================================
# 6Ô∏è‚É£ Run Optimization
# =====================================================
best_params = evolve_population(df, pop_size=4, generations=3)
print("\nüèÅ Best parameters found:", best_params)

# =====================================================
# 7Ô∏è‚É£ Retrain with Best Params (no leakage)
# =====================================================
lstm1_units, lstm2_units, dense_units = (
    best_params['lstm1_units'], best_params['lstm2_units'], best_params['dense_units']
)

_,y_test_simple_true,scaler_y,_,_,_,_,X_train_simple,y_train_simple,X_test_simple,y_test_simple = train_test() 

k = 1
model = build_attlstm_model((SEQ_LEN, K), lstm1_units, lstm2_units, dense_units)
model.summary()

history_attlstm_simple = model.fit(
    X_train_simple, y_train_simple,
    validation_data=(X_test_simple, y_test_simple),
    epochs=EPOCHS_FINAL,
    batch_size=BATCH_SIZE,
    verbose=1
)

# Predict (inverse-transform from original OT scaling)
y_pred_attlstm_simple_ga_scaled = model.predict(X_test_simple)
y_pred_attlstm_simple_ga_true = scaler_y.inverse_transform(y_pred_attlstm_simple_ga_scaled)

# =====================================================
# 8Ô∏è‚É£ Visualization
# =====================================================
plt.figure(figsize=(8,4))
plt.plot(history_attlstm_simple.history['loss'], label='Train Loss')
plt.plot(history_attlstm_simple.history['val_loss'], label='Val Loss')
plt.title('Training & Validation Loss (ATT-LSTM + Simple)')
plt.legend(); plt.grid(True); plt.show()

plt.figure(figsize=(12,5))
plt.plot(y_test_simple_true, label='True OT', color='black', linewidth=1)
plt.plot(y_pred_attlstm_simple_ga_true, label='Predicted OT', color='red', alpha=0.8)
plt.title('True vs Predicted OT (Optimized ATT-LSTM + Simple)')
plt.xlabel('Time Steps'); plt.ylabel('OT Value')
plt.legend(); plt.grid(True); plt.show()

plt.figure(figsize=(10,4))
residuals = y_test_simple_true - y_pred_attlstm_simple_ga_true
plt.plot(residuals, color='purple')
plt.title('Residuals (True - Predicted)')
plt.grid(True); plt.show()





from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping

_,y_test_simple_true,scaler_y,_,_,_,_,X_train_simple,y_train_simple,X_test_simple,y_test_simple = train_test() 

# Advanced LSTM model
advanced_lstm = Sequential([
    Bidirectional(LSTM(122, return_sequences=True), input_shape=(SEQ_LEN, 1)),
    Dropout(0.2),
    LSTM(122, return_sequences=True),
    LSTM(64, return_sequences=True),
    Dropout(0.2),
    LSTM(122, return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1)
])

advanced_lstm.compile(optimizer='adam', loss='mse')
advanced_lstm.summary()

# Early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train model
history_lstm_simple = advanced_lstm.fit(
    X_train_simple, y_train_simple,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_data=(X_test_simple, y_test_simple),
    callbacks=[early_stop],
    verbose=1
)

# Plot training history
plt.figure(figsize=(8, 4))
plt.plot(history_lstm_simple.history['loss'], label='Training Loss')
plt.plot(history_lstm_simple.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.title('Advanced LSTM Training Performance')
plt.legend()
plt.grid(True)
plt.show()

# Make predictions
y_pred_scaled_lstm = advanced_lstm.predict(X_test_simple)
y_pred_lstm_simple_true = scaler_y.inverse_transform(y_pred_scaled_lstm.reshape(-1, 1)).flatten()

# Plot predictions vs true values
plt.figure(figsize=(12, 5))
plt.plot(y_test_simple_true, label='True OT', color='blue')
plt.plot(y_pred_lstm_simple_true, label='Predicted OT', color='green')
plt.xlabel('Time Steps')
plt.ylabel('Oil Temperature')
plt.title('Advanced LSTM Predictions vs True Values')
plt.legend()
plt.grid(True)
plt.show()






# Code: VMD -> LSTM (no leakage)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from vmdpy import VMD
import tensorflow as tf
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam


_,y_test_vmd_true,_,X_train_vmd, y_train_vmd, X_test_vmd, y_test_vmd,_,_,_,_= train_test(alpha, tau, K)

# ---------------------------
# 5. Build LSTM model
# ---------------------------
def build_lstm_model(input_shape, units=LSTM_UNITS, dropout=DROPOUT):
    inp = Input(shape=input_shape)
    x = LSTM(units, return_sequences=True)(inp)
    x = Dropout(dropout)(x)
    x = LSTM(units, return_sequences=False)(x)
    x = Dropout(dropout)(x)
    out = Dense(1)(x)
    model = Model(inp, out)
    model.compile(optimizer=Adam(LEARNING_RATE), loss='mse', metrics=['mae'])
    return model

model = build_lstm_model((SEQ_LEN, K))
model.summary()

# ---------------------------
# 6. Train
# ---------------------------
history_lstm_vmd = model.fit(
    X_train_vmd, y_train_vmd,
    validation_data=(X_test_vmd, y_test_vmd),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    verbose=1
)

# ---------------------------
# 7. Predict & inverse-transform
# ---------------------------
y_pred_lstm_vmd_scaled = model.predict(X_test_vmd)
y_pred_lstm_vmd_true = scaler_y.inverse_transform(y_pred_lstm_vmd_scaled).flatten()

# ---------------------------
# 9. Plots
# ---------------------------
plt.figure(figsize=(8,4))
plt.plot(history_lstm_vmd.history['loss'], label='train loss')
plt.plot(history_lstm_vmd.history['val_loss'], label='val loss')
plt.xlabel('epoch'); plt.ylabel('mse'); plt.legend(); plt.grid(True)
plt.title('Train / Val Loss')
plt.show()

plt.figure(figsize=(12,5))
plt.plot(y_test_simple_true, label='True OT', color='black', linewidth=1)
plt.plot(y_pred_lstm_vmd_true, label='Predicted OT', color='red', alpha=0.8)
plt.xlabel('time steps (test)')
plt.ylabel('OT')
plt.title('True vs Predicted OT (VMD -> LSTM)')
plt.legend(); plt.grid(True)
plt.show()

plt.figure(figsize=(10,4))
res = y_pred_lstm_vmd_true - y_test_vmd_true
plt.plot(res, color='purple')
plt.title('Residuals (True - Pred)')
plt.grid(True)
plt.show()






from tensorflow.keras.layers import Layer, LSTM, Dense, LayerNormalization, Dropout

class XLSTM(Layer):
    def __init__(self, units, dropout=0.1, **kwargs):
        super(XLSTM, self).__init__(**kwargs)
        self.units = units
        self.dropout_rate = dropout

        self.lstm = LSTM(units, return_sequences=True)
        self.dropout_layer = Dropout(dropout)
        self.norm = LayerNormalization()

    def build(self, input_shape):
        input_dim = input_shape[-1]    # <-- Detect feature count automatically

        # Projection for residual connection
        self.proj = Dense(self.units)  # input_dim ‚Üí units

        super().build(input_shape)

    def call(self, x):
        h = self.lstm(x)
        h = self.dropout_layer(h)

        h_res = self.proj(x)   # Now works for any feature dimension

        return self.norm(h + h_res)


# =============================
# XLSTM Model
# =============================
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM
from tensorflow.keras.callbacks import EarlyStopping

# Load data
_,y_test_simple_true,scaler_y,_,_,_,_,X_train_simple,y_train_simple,X_test_simple,y_test_simple = train_test()

# -----------------------------
# XLSTM Model Architecture
# -----------------------------

xlstm_model = Sequential([
    Input(shape=(SEQ_LEN, X_train_vmd.shape[-1])),  # automatic feature count
    XLSTM(128, dropout=0.2),
    XLSTM(128, dropout=0.2),
    XLSTM(64, dropout=0.2),
    LSTM(64, return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1)
])

xlstm_model.compile(optimizer='adam', loss='mse')
xlstm_model.summary()

# Early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train XLSTM
history_xlstm_vmd = xlstm_model.fit(
    X_train_vmd, y_train_vmd,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_data=(X_test_vmd, y_test_vmd),
    callbacks=[early_stop],
    verbose=1
)

plt.figure(figsize=(8, 4))
plt.plot(history_xlstm_vmd.history['loss'], label='Training Loss')
plt.plot(history_xlstm_vmd.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.title('XLSTM VMD Training Performance')
plt.legend()
plt.grid(True)
plt.show()

y_pred_xlstm_vmd_scaled = xlstm_model.predict(X_test_vmd)
y_pred_xlstm_vmd_true = scaler_y.inverse_transform(y_pred_xlstm_vmd_scaled.reshape(-1, 1)).flatten()

plt.figure(figsize=(12, 5))
plt.plot(y_test_vmd_true, label='True OT', color='blue')
plt.plot(y_pred_xlstm_vmd_true, label='XLSTM_VMD Predicted OT', color='red')
plt.xlabel('Time Steps')
plt.ylabel('Oil Temperature')
plt.title('XLSTM Predictions vs True Values')
plt.legend()
plt.grid(True)
plt.show()


plt.figure(figsize=(10,4))
residuals = y_test_vmd_true - y_pred_xlstm_vmd_true
plt.plot(residuals, color='purple')
plt.title('Residuals (True - Predicted)')
plt.grid(True); plt.show()






from tensorflow.keras.layers import Layer, LSTM, Dense, LayerNormalization, Dropout

class XLSTM(Layer):
    def __init__(self, units, dropout=0.1, **kwargs):
        super(XLSTM, self).__init__(**kwargs)
        self.units = units
        self.dropout_rate = dropout

        self.lstm = LSTM(units, return_sequences=True)
        self.dropout_layer = Dropout(dropout)
        self.norm = LayerNormalization()

    def build(self, input_shape):
        input_dim = input_shape[-1]    # <-- Detect feature count automatically

        # Projection for residual connection
        self.proj = Dense(self.units)  # input_dim ‚Üí units

        super().build(input_shape)

    def call(self, x):
        h = self.lstm(x)
        h = self.dropout_layer(h)

        h_res = self.proj(x)   # Now works for any feature dimension

        return self.norm(h + h_res)


# =============================
# XLSTM Model
# =============================
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM
from tensorflow.keras.callbacks import EarlyStopping

# Load data
_,y_test_simple_true,scaler_y,_,_,_,_,X_train_simple,y_train_simple,X_test_simple,y_test_simple = train_test()

# -----------------------------
# XLSTM Model Architecture
# -----------------------------

xlstm_model_simple = Sequential([
    Input(shape=(SEQ_LEN, X_train_simple.shape[-1])), # automatic feature count
    XLSTM(128, dropout=0.2),
    XLSTM(128, dropout=0.2),
    XLSTM(64, dropout=0.2),
    LSTM(64, return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1)
])

xlstm_model_simple.compile(optimizer='adam', loss='mse')
xlstm_model_simple.summary()

# Early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
# Train XLSTM
history_xlstm_simple = xlstm_model_simple.fit(
    X_train_simple, y_train_simple,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_data=(X_test_simple, y_test_simple),
    callbacks=[early_stop],
    verbose=1
)

plt.figure(figsize=(8, 4))
plt.plot(history_xlstm_simple.history['loss'], label='Training Loss')
plt.plot(history_xlstm_simple.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.title('XLSTM SIMPLE Training Performance')
plt.legend()
plt.grid(True)
plt.show()

y_pred_xlstm_simple_scaled = xlstm_model_simple.predict(X_test_simple)
y_pred_xlstm_simple_true = scaler_y.inverse_transform(y_pred_xlstm_simple_scaled.reshape(-1, 1)).flatten()

plt.figure(figsize=(12, 5))
plt.plot(y_test_simple_true, label='True OT', color='blue')
plt.plot(y_pred_xlstm_simple_true, label='XLSTM_SIMPLE Predicted OT', color='red')
plt.xlabel('Time Steps')
plt.ylabel('Oil Temperature')
plt.title('XLSTM Predictions vs True Values')
plt.legend()
plt.grid(True)
plt.show()


plt.figure(figsize=(10,4))
residuals = y_test_simple_true - y_pred_xlstm_simple_true
plt.plot(residuals, color='purple')
plt.title('Residuals (True - Predicted)')
plt.grid(True); plt.show()






class XLSTM(Layer):
    def __init__(self, units, dropout=0.1, **kwargs):
        super(XLSTM, self).__init__(**kwargs)
        self.units = units
        self.dropout_rate = dropout

        self.lstm = LSTM(units, return_sequences=True)
        self.dropout_layer = Dropout(dropout)
        self.norm = LayerNormalization()

    def build(self, input_shape):
        input_dim = input_shape[-1]    # <-- Detect feature count automatically

        # Projection for residual connection
        self.proj = Dense(self.units)  # input_dim ‚Üí units

        super().build(input_shape)

    def call(self, x):
        h = self.lstm(x)
        h = self.dropout_layer(h)

        h_res = self.proj(x)   # Now works for any feature dimension

        return self.norm(h + h_res)

# Get final train/test split (no leakage)
y_test_vmd_true,_,_,X_train_vmd, y_train_vmd, X_test_vmd, y_test_vmd,_,_,_,_= train_test(alpha, tau, K)

input_shape = (SEQ_LEN, X_train_vmd.shape[-1])
inp = Input(shape=input_shape)
x = XLSTM(128, dropout=0.2)(inp)
x = XLSTM(128, dropout=0.2)(x)
x = XLSTM(64, dropout=0.2)(x)

    # Self-Attention Layer
query = Dense(128)(x)
value = Dense(128)(x)
attention_out = Attention()([query, value])

    # Concatenate XLSTM + Attention output
attention_out = Concatenate()([x, attention_out])

    # Decoder LSTM
lstm_dec = LSTM(lstm2_units, return_sequences=False)(attention_out)
dense_out = Dense(64, activation='relu')(lstm_dec)
final_out = Dense(1)(dense_out)

model_xlstm_attlstm_vmd = Model(inputs=inp, outputs=final_out)
model_xlstm_attlstm_vmd.compile(optimizer='Adam', loss='mse', metrics=['mae'])

model_xlstm_attlstm_vmd.summary()

# Train final model
history_xlstm_attlstm_vmd = model_xlstm_attlstm_vmd.fit(
    X_train_vmd, y_train_vmd,
    validation_data=(X_test_vmd, y_test_vmd),
    epochs=EPOCHS_FINAL,
    batch_size=BATCH_SIZE,
    verbose=1
)

# Predict (inverse-transform from original OT scaling)
y_pred_xlstm_attlstm_vmd_scaled = model_xlstm_attlstm_vmd.predict(X_test_vmd)
y_pred_xlstm_attlstm_vmd_true = scaler_y.inverse_transform(y_pred_xlstm_attlstm_vmd_scaled)

# =====================================================
# 8Ô∏è‚É£ Visualization
# =====================================================
plt.figure(figsize=(8,4))
plt.plot(history_xlstm_attlstm_vmd.history['loss'], label='Train Loss')
plt.plot(history_xlstm_attlstm_vmd.history['val_loss'], label='Val Loss')
plt.title('Training & Validation Loss (ATT-XLSTM + VMD )')
plt.legend(); plt.grid(True); plt.show()

plt.figure(figsize=(12,5))
plt.plot(y_test_vmd_true, label='True OT', color='black', linewidth=1)
plt.plot(y_pred_xlstm_attlstm_vmd_true, label='Predicted OT', color='red', alpha=0.8)
plt.title('True vs Predicted OT (Optimized ATT-XLSTM + VMD )')
plt.xlabel('Time Steps'); plt.ylabel('OT Value')
plt.legend(); plt.grid(True); plt.show()

plt.figure(figsize=(10,4))
residuals = y_test_vmd_true - y_pred_xlstm_attlstm_vmd_true
plt.plot(residuals, color='purple')
plt.title('Residuals (True - Predicted)')
plt.grid(True); plt.show()





# =========================================================
# üìå 1 ‚Äî XLSTM (unchanged)
# =========================================================
class XLSTM(Layer):
    def __init__(self, units, dropout=0.1, **kwargs):
        super(XLSTM, self).__init__(**kwargs)
        self.units = units
        self.dropout_rate = dropout

        self.lstm = LSTM(units, return_sequences=True)
        self.dropout_layer = Dropout(dropout)
        self.norm = LayerNormalization()

    def build(self, input_shape):
        input_dim = input_shape[-1]
        self.proj = Dense(self.units)
        super().build(input_shape)

    def call(self, x):
        h = self.lstm(x)
        h = self.dropout_layer(h)
        h_res = self.proj(x)
        return self.norm(h + h_res)


# =========================================================
# üìå 2 ‚Äî Custom Luong Attention
# score = h_t ¬∑ W ¬∑ h_s  (dot-product)
# =========================================================
class LuongAttention(Layer):
    def __init__(self, units):
        super(LuongAttention, self).__init__()
        self.Wq = Dense(units)
        self.Wv = Dense(units)

    def call(self, query, value, mask=None):
        # Project both ‚Üí SAME dimension
        q = self.Wq(query)      # (B, T, units)
        v = self.Wv(value)      # (B, T, units)

        # Dot product score
        score = tf.matmul(q, v, transpose_b=True)  # (B, T, T)

        if mask is not None:
            score += (mask * -1e9)

        # Use tf.nn.softmax (Keras-safe)
        attn_weights = tf.nn.softmax(score, axis=-1)     # (B, T, T)
        context = tf.matmul(attn_weights, v)             # (B, T, units)

        return context


        
# =========================================================
# üìå 3 ‚Äî Temporal Attention (per timestep)
# =========================================================
class TemporalAttention(Layer):
    def __init__(self, units):
        super().__init__()
        self.Wt = Dense(units)
        self.vt = Dense(1)

    def call(self, x):
        h = tf.nn.tanh(self.Wt(x))
        score = self.vt(h)
        alpha = tf.nn.softmax(score, axis=1)
        context = tf.reduce_sum(alpha * x, axis=1)
        return context


# =========================================================
# üìå 4 ‚Äî Cross-Attention (Query from XLSTM, Key/Value from VMD input)
# =========================================================
class CrossAttention(Layer):
    def __init__(self, num_heads=4, key_dim=32):
        super().__init__()
        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)

    def call(self, query, context, mask=None):
        return self.mha(query=query, value=context, key=context, attention_mask=mask)


# =========================================================
# üìå 5 ‚Äî Causal Masking (prevent future leakage)
# =========================================================
def causal_mask(seq_len):
    mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)
    return mask[None, None, :, :]


# =========================================================
# üìå 6 ‚Äî THE FINAL mehdiy_v2 MODEL
# =========================================================
def build_mehdiy_v2(input_shape, lstm2_units=64):

    seq_len = input_shape[0]
    inp = Input(shape=input_shape)

    # -------------------------------
    # (A) XLSTM BLOCKS
    # -------------------------------
    x = XLSTM(128, dropout=0.2)(inp)
    x = XLSTM(128, dropout=0.2)(x)
    x = XLSTM(64, dropout=0.2)(x)

    # =====================================================
    # (B) Attention Upgrade Pack
    # =====================================================

    # -------------------------------
    # 1) LUONG Attention
    # -------------------------------
    luong = LuongAttention(128)(x, x)

    # -------------------------------
    # 2) Multi-Head Attention with causal mask
    # -------------------------------
    mask = causal_mask(seq_len)
    mha = MultiHeadAttention(num_heads=4, key_dim=32)(x, x, attention_mask=mask)

    # -------------------------------
    # 3) Temporal Attention
    # -------------------------------
    temp_context = TemporalAttention(64)(x)
    temp_context = RepeatVector(seq_len)(temp_context)

    # -------------------------------
    # 4) Cross Attention (input ‚Üí XLSTM)
    # -------------------------------
    cross = CrossAttention(num_heads=4, key_dim=32)(x, inp)

    # -------------------------------
    # COMBINE ALL ATTENTIONS
    # -------------------------------
    att_concat = Concatenate()([x, luong, mha, temp_context, cross])

    # -------------------------------
    # Decoder
    # -------------------------------
    lstm_dec = LSTM(lstm2_units, return_sequences=False)(att_concat)
    dense = Dense(64, activation='relu')(lstm_dec)
    out = Dense(1)(dense)

    return Model(inputs=inp, outputs=out)


# =================================================================
# üìå 7 ‚Äî Training (same style as your mehdiy code)
# =================================================================
# Final non-leaking train/test
y_test_vmd_true, _, _, X_train_vmd, y_train_vmd, X_test_vmd, y_test_vmd, _, _, _, _ = train_test(alpha, tau, K)

input_shape = (SEQ_LEN, X_train_vmd.shape[-1])
model_mixed = build_mehdiy_v2(input_shape=input_shape, lstm2_units=lstm2_units)

model_mixed.compile(optimizer='adam', loss='mse', metrics=['mae'])
model_mixed.summary()

history_model_mixed = model_mixed.fit(
    X_train_vmd, y_train_vmd,
    validation_data=(X_test_vmd, y_test_vmd),
    epochs=EPOCHS_FINAL,
    batch_size=BATCH_SIZE,
    verbose=1
)

# Predictions
y_pred_model_mixed_scaled = model_mixed.predict(X_test_vmd)
y_pred_model_mixed_true = scaler_y.inverse_transform(y_pred_model_mixed_scaled)


# =================================================================
# üìå 8 ‚Äî PLOTTING (same format as mehdiy)
# =================================================================
plt.figure(figsize=(8, 4))
plt.plot(history_model_mixed.history['loss'], label='Train Loss')
plt.plot(history_model_mixed.history['val_loss'], label='Val Loss')
plt.title('Training & Validation Loss (mixed)')
plt.legend(); plt.grid(True); plt.show()

plt.figure(figsize=(12, 5))
plt.plot(y_test_vmd_true, label='True OT', color='black')
plt.plot(y_pred_model_mixed_true, label='Predicted OT (mixed)', color='red')
plt.title('True vs Predicted OT ‚Äî mixed')
plt.legend(); plt.grid(True); plt.show()

plt.figure(figsize=(10, 4))
residuals = y_test_vmd_true - y_pred_model_mixed_true.flatten()
plt.plot(residuals, color='purple')
plt.title('Residuals')
plt.grid(True); plt.show()






from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

def compute_metrics(y_true, y_pred):
    min_len = min(len(y_true), len(y_pred))
    y_true, y_pred = y_true[:min_len], y_pred[:min_len]
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return mse, rmse, mae, r2

# Compute metrics
mse_Attlstm_vmd_ga, rmse_Attlstm_vmd_ga, mae_Attlstm_vmd_ga, r2_attlstm_vmd_ga = compute_metrics(y_pred_attlstm_vmd_ga_true, y_test_vmd_true)
mse_Lstm_simple, rmse_Lstm_simple, mae_Lstm_simple, r2_Lstm_simple = compute_metrics(y_pred_lstm_simple_true, y_test_simple_true)
mse_Attlstm_simple_ga, rmse_Attlstm_simple_ga, mae_Attlstm_simple_ga, r2_attlstm_simple_ga = compute_metrics(y_pred_attlstm_simple_ga_true, y_test_simple_true)
mse_Lstm_vmd, rmse_Lstm_vmd, mae_Lstm_vmd, r2_Lstm_vmd = compute_metrics(y_pred_lstm_vmd_true, y_test_vmd_true)
mse_XLstm_vmd, rmse_XLstm_vmd, mae_XLstm_vmd, r2_XLstm_vmd = compute_metrics(y_pred_xlstm_vmd_true, y_test_vmd_true)
mse_XLstm_simple, rmse_XLstm_simple, mae_XLstm_simple, r2_XLstm_simple = compute_metrics(y_pred_xlstm_simple_true, y_test_simple_true)
mse_AttLstm_XLstm_vmd, rmse_AttLstm_XLstm_vmd, mae_AttLstm_XLstm_vmd, r2_AttLstm_XLstm_vmd = compute_metrics(y_pred_xlstm_attlstm_vmd_true, y_test_vmd_true)
mse_mixed, rmse_mixed, mae_mixed, r2_mixed = compute_metrics(y_pred_model_mixed_true, y_test_vmd_true)

# Print comparison
print("üìà Model Performance Comparison")
print(f"ATT-LSTM-VMD-GA: MSE={mse_Attlstm_vmd_ga:.4f}, RMSE={rmse_Attlstm_vmd_ga:.4f}, MAE={mae_Attlstm_vmd_ga:.4f}, R¬≤={r2_attlstm_vmd_ga:.4f}")
print(f"Lstm_Simple: MSE={mse_Lstm_simple:.4f}, RMSE={rmse_Lstm_simple:.4f}, MAE={mae_Lstm_simple:.4f}, R¬≤={r2_Lstm_simple:.4f}")
print(f"Attlstm_Simple_GA: MSE={mse_Attlstm_simple_ga:.4f}, RMSE={rmse_Attlstm_simple_ga:.4f}, MAE={mae_Attlstm_simple_ga:.4f}, R¬≤={r2_attlstm_simple_ga:.4f}")
print(f"Lstm_vmd: MSE={mse_Lstm_vmd:.4f}, RMSE={rmse_Lstm_vmd:.4f}, MAE={mae_Lstm_vmd:.4f}, R¬≤={r2_Lstm_vmd:.4f}")
print(f"XLstm_vmd: MSE={mse_XLstm_vmd:.4f}, RMSE={rmse_XLstm_vmd:.4f}, MAE={mae_XLstm_vmd:.4f}, R¬≤={r2_XLstm_vmd:.4f}")
print(f"XLstm_simple: MSE={mse_XLstm_simple:.4f}, RMSE={rmse_XLstm_simple:.4f}, MAE={mae_XLstm_simple:.4f}, R¬≤={r2_XLstm_simple:.4f}")
print(f"AttLSTM_XLSTM_VMD: MSE={mse_AttLstm_XLstm_vmd:.4f}, RMSE={rmse_AttLstm_XLstm_vmd:.4f}, MAE={mae_AttLstm_XLstm_vmd:.4f}, R¬≤={r2_AttLstm_XLstm_vmd:.4f}")
print(f"MIXED: MSE={mse_mixed:.4f}, RMSE={rmse_mixed:.4f}, MAE={mae_mixed:.4f}, R¬≤={r2_mixed:.4f}")



diff = y_train_simple - y_train_vmd
plt.figure(figsize=(10,4))
plt.plot(diff, color='orange')
plt.title('Difference: y_train_simple vs y_train_vmd')
plt.xlabel('Time steps')
plt.ylabel('Temperature difference')
plt.grid(True)
plt.show()

y_test_vmd_true = scaler_y.inverse_transform(y_test_vmd)
y_test_simple_true = scaler_y.inverse_transform(y_test_simple)
diff = y_test_simple_true - y_test_vmd_true
plt.figure(figsize=(10,4))
plt.plot(diff, color='orange')
plt.title('Difference: VMD-test-true vs Simple-test-true of OT')
plt.xlabel('Time steps')
plt.ylabel('Temperature difference')
plt.grid(True)
plt.show()
