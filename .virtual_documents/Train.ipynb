





# Parameters
SEQ_LEN = 24  # past 24 hours for each sample
TARGET_COL = 'OT'  # target variable: Oil Temperature
EPOCHS = 30 
BATCH_SIZE = 32

# VMD parameters
alpha = 2000       # moderate bandwidth constraint
tau = 0            # noise-tolerance (0 for no strict fidelity)
K = 30              # number of modes to extract
DC = 0              # no DC part imposed
init = 1            # initialize omegas uniformly
tol = 1e-7


import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import os
from vmdpy import VMD
import pandas as pd
from sklearn.preprocessing import MinMaxScaler


save_dir = os.path.expanduser("~/Project/AttnLstm/data/raw")
os.makedirs(save_dir, exist_ok=True)
# Full file path
file_path = os.path.join(save_dir, "ETTh1.csv")
# Load it whenever needed
df = pd.read_csv(file_path)
print(df.head())
df.columns = df.columns.str.strip().str.replace('\ufeff', '')
print(df.columns)
# Ensure datetime type
df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)
data = df[[TARGET_COL]].values



# Visualize
plt.figure(figsize=(12, 4))
plt.plot(df.index, df['OT'], label='Oil Temperature (OT)')
plt.title('ETTh1 - Oil Temperature Time Series')
plt.xlabel('Date')
plt.ylabel('OT')
plt.legend()
plt.show()





# Select relevant columns
df_model = df[['OT']].copy()

# Split index (80% train, 20% test)
split_idx = int(len(df_model) * 0.8)

# Split data chronologically
train_data = df_model[:split_idx]
test_data  = df_model[split_idx:]

# Fit scaler only on training data
scaler = MinMaxScaler()
train_scaled = scaler.fit_transform(train_data)
test_scaled  = scaler.transform(test_data)

# Sequence creation function
def create_sequences(data, seq_len):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i + seq_len])
        y.append(data[i + seq_len, 0])  # target: OT
    return np.array(X), np.array(y)

# Create sequences
X_train, y_train = create_sequences(train_scaled, SEQ_LEN)
X_test, y_test   = create_sequences(test_scaled, SEQ_LEN)

# If using pure LSTM on OT only
X_train_simple = X_train[:, :, 0:1]
X_test_simple  = X_test[:, :, 0:1]

print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")









import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Dense, Attention, Concatenate, Lambda, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import random

# -----------------------------
# 1. Genetic Algorithm Configuration
# -----------------------------
POP_SIZE = 10       # number of candidate solutions
N_GEN = 5           # generations
MUTATION_RATE = 0.3 # probability of mutation

# Define search spaces
param_space = {
    # VMD parameters
    "alpha": (100, 5000),
    "K": (3, 10),
    "tau": (0.0, 0.5),
    # Model parameters
    "lstm_units": (32, 128),
    "learning_rate": (1e-4, 1e-2),
    "dropout": (0.0, 0.5)
}

def random_params():
    """Generate a random parameter set."""
    return {
        "alpha": np.random.uniform(*param_space["alpha"]),
        "K": np.random.randint(*param_space["K"]),
        "tau": np.random.uniform(*param_space["tau"]),
        "lstm_units": np.random.randint(*param_space["lstm_units"]),
        "learning_rate": 10**np.random.uniform(np.log10(param_space["learning_rate"][0]),
                                               np.log10(param_space["learning_rate"][1])),
        "dropout": np.random.uniform(*param_space["dropout"])
    }

# -----------------------------
# 2. Fitness Function
# -----------------------------
def evaluate(params):
    """Evaluate MSE of model trained with given parameters."""

    alpha, tau, K = params["alpha"], params["tau"], params["K"]

    # --- Apply VMD ---
    signal = data.flatten()
    u, u_hat, omega = VMD(signal, alpha, tau, K, DC=0, init=1, tol=1e-7)
    vmd_features = np.stack(u, axis=1)

    # Split and scale
    split_idx = int(len(vmd_features) * 0.8)
    scaler = MinMaxScaler()
    train_scaled = scaler.fit_transform(vmd_features[:split_idx])
    test_scaled  = scaler.transform(vmd_features[split_idx:])

    def create_sequences(data, seq_len):
        X, y = [], []
        for i in range(len(data) - seq_len):
            X.append(data[i:i + seq_len])
            y.append(data[i + seq_len, 0])
        return np.array(X), np.array(y)

    X_train, y_train = create_sequences(train_scaled, SEQ_LEN)
    X_test, y_test = create_sequences(test_scaled, SEQ_LEN)

    # --- Build model ---
    inputs = Input(shape=(SEQ_LEN, K))
    x = LSTM(params["lstm_units"], return_sequences=True)(inputs)
    x = Dropout(params["dropout"])(x)
    x = LSTM(params["lstm_units"], return_sequences=True)(x)
    lstm_out = LSTM(params["lstm_units"], return_sequences=True)(x)

    # Attention
    context = Attention()([lstm_out, lstm_out])
    last_output = Lambda(lambda x: x[:, -1, :])(lstm_out)
    context_mean = Lambda(lambda x: tf.reduce_mean(x, axis=1))(context)
    concat = Concatenate()([last_output, context_mean])
    dense_out = Dense(1)(concat)

    model = Model(inputs, dense_out)
    model.compile(optimizer=Adam(learning_rate=params["learning_rate"]), loss="mse")

    # Train briefly (for fitness estimation)
    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)
    preds = model.predict(X_test)
    mse = mean_squared_error(y_test, preds)

    return mse

# -----------------------------
# 3. GA Loop
# -----------------------------
population = [random_params() for _ in range(POP_SIZE)]

for gen in range(N_GEN):
    print(f"\n--- Generation {gen+1}/{N_GEN} ---")

    # Evaluate fitness
    fitness = []
    for individual in population:
        mse = evaluate(individual)
        fitness.append(mse)
        print(f"Params: {individual} â†’ MSE: {mse:.6f}")

    # Select top 50%
    ranked = [x for _, x in sorted(zip(fitness, population), key=lambda pair: pair[0])]
    survivors = ranked[:POP_SIZE // 2]

    # Generate offspring
    offspring = []
    while len(offspring) < POP_SIZE // 2:
        p1, p2 = random.sample(survivors, 2)
        child = {}
        for key in param_space:
            child[key] = random.choice([p1[key], p2[key]])
            # Mutation
            if random.random() < MUTATION_RATE:
                if isinstance(param_space[key][0], int):
                    child[key] = np.random.randint(*param_space[key])
                else:
                    child[key] = np.random.uniform(*param_space[key])
        offspring.append(child)

    # Next generation
    population = survivors + offspring

# Final best parameters
best_params = min(population, key=lambda p: evaluate(p))
print("\nâœ… Best Parameters Found:")
print(best_params)






# -----------------------------
# Final Training with Best Parameters
# -----------------------------
alpha, tau, K = best_params["alpha"], best_params["tau"], best_params["K"]
lstm_units = best_params["lstm_units"]
dropout = best_params["dropout"]
lr = best_params["learning_rate"]

# --- Apply VMD with optimal parameters ---
signal = data.flatten()
u, u_hat, omega = VMD(signal, alpha, tau, K, DC=0, init=1, tol=1e-7)
vmd_features = np.stack(u, axis=1)

# Train/test split
split_idx = int(len(vmd_features) * 0.8)
scaler = MinMaxScaler()
train_scaled = scaler.fit_transform(vmd_features[:split_idx])
test_scaled  = scaler.transform(vmd_features[split_idx:])

def create_sequences(data, seq_len):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i + seq_len])
        y.append(data[i + seq_len, 0])
    return np.array(X), np.array(y)

X_train, y_train = create_sequences(train_scaled, SEQ_LEN)
X_test, y_test = create_sequences(test_scaled, SEQ_LEN)

# --- Build best model ---
inputs = Input(shape=(SEQ_LEN, K))
x = LSTM(lstm_units, return_sequences=True)(inputs)
x = Dropout(dropout)(x)
x = LSTM(lstm_units, return_sequences=True)(x)
x = Dropout(dropout)(x)
lstm_out = LSTM(lstm_units, return_sequences=True)(x)

context = Attention()([lstm_out, lstm_out])
last_output = Lambda(lambda x: x[:, -1, :])(lstm_out)
context_mean = Lambda(lambda x: tf.reduce_mean(x, axis=1))(context)
concat = Concatenate()([last_output, context_mean])
dense_out = Dense(1)(concat)

model = Model(inputs, dense_out)
model.compile(optimizer=Adam(learning_rate=lr), loss='mse')

# --- Train final model ---
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    verbose=1
)

# -----------------------------
# 1ï¸âƒ£ Training Loss Plot
# -----------------------------
plt.figure(figsize=(8, 4))
plt.plot(history.history['loss'], label='Training Loss', color='blue')
plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)
plt.show()

# -----------------------------
# 2ï¸âƒ£ Prediction vs True Values
# -----------------------------
y_pred = model.predict(X_test)

plt.figure(figsize=(12, 5))
plt.plot(y_test, label='True Values', color='green')
plt.plot(y_pred, label='Predicted Values', color='red', alpha=0.7)
plt.xlabel('Time Steps')
plt.ylabel('Target')
plt.title('True vs Predicted Values')
plt.legend()
plt.grid(True)
plt.show()

# -----------------------------
# 3ï¸âƒ£ Prediction Error (Residuals)
# -----------------------------
errors = y_test - y_pred.flatten()

plt.figure(figsize=(8, 4))
plt.plot(errors, color='purple')
plt.title('Prediction Errors (Residuals)')
plt.xlabel('Time Step')
plt.ylabel('Error')
plt.grid(True)
plt.show()

# Optional: Histogram of residuals
plt.figure(figsize=(6, 4))
plt.hist(errors, bins=30, color='gray', edgecolor='black')
plt.title('Error Distribution')
plt.xlabel('Error')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Report Metrics
mse = np.mean(errors**2)
mae = np.mean(np.abs(errors))
rmse = np.sqrt(mse)

print(f"\nðŸ“Š Model Evaluation:")
print(f"MSE  = {mse:.6f}")
print(f"MAE  = {mae:.6f}")
print(f"RMSE = {rmse:.6f}")



# --- Evaluate ---
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"\nâœ… Final Model MSE: {mse:.6f}")



from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Attention, Concatenate, Lambda
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

# -----------------------------
# 1ï¸âƒ£ Apply Optimal VMD Parameters
# -----------------------------
alpha, tau, K = best_params["alpha"], best_params["tau"], best_params["K"]
lstm_units = best_params["lstm_units"]
dropout = best_params["dropout"]
lr = best_params["learning_rate"]

signal = data.flatten()
u, u_hat, omega = VMD(signal, alpha, tau, K, DC=0, init=1, tol=1e-7)
vmd_features = np.stack(u, axis=1)  # shape: (time, K)

# -----------------------------
# 2ï¸âƒ£ Train/Test Split
# -----------------------------
split_idx = int(len(vmd_features) * 0.8)
scaler = MinMaxScaler()
train_scaled = scaler.fit_transform(vmd_features[:split_idx])
test_scaled  = scaler.transform(vmd_features[split_idx:])

def create_sequences(data, seq_len):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i + seq_len])
        y.append(data[i + seq_len])  # all K IMFs at once
    return np.array(X), np.array(y)

X_train, y_train = create_sequences(train_scaled, SEQ_LEN)
X_test, y_test = create_sequences(test_scaled, SEQ_LEN)

# -----------------------------
# 3ï¸âƒ£ Build Best Model
# -----------------------------
inputs = Input(shape=(SEQ_LEN, K))
x = LSTM(lstm_units, return_sequences=True)(inputs)
x = Dropout(dropout)(x)
x = LSTM(lstm_units, return_sequences=True)(x)
x = Dropout(dropout)(x)
lstm_out = LSTM(lstm_units, return_sequences=True)(x)

context = Attention()([lstm_out, lstm_out])
last_output = Lambda(lambda x: x[:, -1, :])(lstm_out)
context_mean = Lambda(lambda x: tf.reduce_mean(x, axis=1))(context)
concat = Concatenate()([last_output, context_mean])
dense_out = Dense(K)(concat)  # predict all IMFs

model = Model(inputs, dense_out)
model.compile(optimizer=Adam(learning_rate=lr), loss='mse')
model.summary()

# -----------------------------
# 4ï¸âƒ£ Train the Model
# -----------------------------
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    verbose=1
)

# -----------------------------
# 5ï¸âƒ£ Loss Plot
# -----------------------------
plt.figure(figsize=(8, 4))
plt.plot(history.history['loss'], label='Train Loss', color='blue')
plt.plot(history.history['val_loss'], label='Val Loss', color='orange')
plt.xlabel('Epochs')
plt.ylabel('MSE')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)
plt.show()

# -----------------------------
# 6ï¸âƒ£ Predict IMFs and Reconstruct OT
# -----------------------------
y_pred_imfs = model.predict(X_test)

# Inverse transform back to original IMF scale
y_pred_imfs_rescaled = scaler.inverse_transform(y_pred_imfs)
y_test_rescaled = scaler.inverse_transform(y_test)

# Reconstruct real OT by summing IMFs
y_pred_ot = np.sum(y_pred_imfs_rescaled, axis=1)
y_true_ot = np.sum(y_test_rescaled, axis=1)

# -----------------------------
# 7ï¸âƒ£ Visualization
# -----------------------------
plt.figure(figsize=(12, 5))
plt.plot(y_true_ot, label='True OT', color='green')
plt.plot(y_pred_ot, label='Predicted OT', color='red', alpha=0.7)
plt.xlabel('Time Steps')
plt.ylabel('Oil Temperature')
plt.title('Reconstructed Real OT (from VMD + Attention-LSTM)')
plt.legend()
plt.grid(True)
plt.show()

# -----------------------------
# 8ï¸âƒ£ Error & Metrics
# -----------------------------
errors = y_true_ot - y_pred_ot
plt.figure(figsize=(8, 4))
plt.plot(errors, color='purple')
plt.title('Reconstruction Error (OT_true - OT_pred)')
plt.xlabel('Time Step')
plt.ylabel('Error')
plt.grid(True)
plt.show()

# Histogram of residuals
plt.figure(figsize=(6, 4))
plt.hist(errors, bins=30, color='gray', edgecolor='black')
plt.title('Error Distribution')
plt.xlabel('Error')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# -----------------------------
# 9ï¸âƒ£ Final Evaluation
# -----------------------------
mse = mean_squared_error(y_true_ot, y_pred_ot)
mae = mean_absolute_error(y_true_ot, y_pred_ot)
rmse = np.sqrt(mse)

print(f"\nðŸ“Š Final Reconstructed OT Metrics:")
print(f"MSE  = {mse:.6f}")
print(f"MAE  = {mae:.6f}")
print(f"RMSE = {rmse:.6f}")






from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping

# Advanced LSTM model
advanced_lstm = Sequential([
    Bidirectional(LSTM(64, return_sequences=True), input_shape=(SEQ_LEN, 1)),
    Dropout(0.2),
    LSTM(128, return_sequences=True),
    LSTM(64, return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1)
])

advanced_lstm.compile(optimizer='adam', loss='mse')
advanced_lstm.summary()

# Early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train model
history_adv = advanced_lstm.fit(
    X_train_simple, y_train,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_data=(X_test_simple, y_test),
    callbacks=[early_stop],
    verbose=1
)

# Plot training history
plt.figure(figsize=(8, 4))
plt.plot(history_adv.history['loss'], label='Training Loss')
plt.plot(history_adv.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.title('Advanced LSTM Training Performance')
plt.legend()
plt.grid(True)
plt.show()

# Make predictions
y_pred_adv = advanced_lstm.predict(X_test_simple)

# Plot predictions vs true values
plt.figure(figsize=(12, 5))
plt.plot(y_test, label='True OT', color='blue')
plt.plot(y_pred_adv, label='Predicted OT', color='green')
plt.xlabel('Time Steps')
plt.ylabel('Oil Temperature')
plt.title('Advanced LSTM Predictions vs True Values')
plt.legend()
plt.grid(True)
plt.show()



from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np
import matplotlib.pyplot as plt

# ----------------------------------------------------
# Ensure y_test is 1D (in case it has multiple columns)
# ----------------------------------------------------
if y_test.ndim > 1:
    y_test_single = y_test[:, 0]   # select the main target column
else:
    y_test_single = y_test

# Flatten predictions if needed
y_pred_vmd = y_pred.flatten()
y_pred_adv = y_pred_adv.flatten()

# ----------------------------------------------------
# Compute Metrics for Both Models
# ----------------------------------------------------
def compute_metrics(y_true, y_pred):
    mse  = mean_squared_error(y_true, y_pred)
    mae  = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2   = r2_score(y_true, y_pred)
    return mse, mae, rmse, r2

mse_vmd, mae_vmd, rmse_vmd, r2_vmd = compute_metrics(y_test_single, y_pred_vmd)
mse_adv, mae_adv, rmse_adv, r2_adv = compute_metrics(y_test_single, y_pred_adv)

# ----------------------------------------------------
# Print Comparison Results
# ----------------------------------------------------
print("\nðŸ“Š Model Performance Comparison:")
print("====================================")
print(f"VMD + Attention-LSTM:")
print(f"  MSE  = {mse_vmd:.6f}")
print(f"  MAE  = {mae_vmd:.6f}")
print(f"  RMSE = {rmse_vmd:.6f}")
print(f"  RÂ²   = {r2_vmd:.6f}")
print("------------------------------------")
print(f"Advanced BiLSTM:")
print(f"  MSE  = {mse_adv:.6f}")
print(f"  MAE  = {mae_adv:.6f}")
print(f"  RMSE = {rmse_adv:.6f}")
print(f"  RÂ²   = {r2_adv:.6f}")

# ----------------------------------------------------
# Plot: True vs Predicted Comparison
# ----------------------------------------------------
plt.figure(figsize=(12, 5))
plt.plot(y_test_single, label='True Values', color='red', linewidth=1)
plt.plot(y_pred_vmd, label='VMD + Attention-LSTM', color='blue', alpha=0.7)
plt.plot(y_pred_adv, label='Advanced BiLSTM', color='green', alpha=0.7)
plt.title('True vs Predicted Values (Model Comparison)')
plt.xlabel('Time Steps')
plt.ylabel('Target (Oil Temperature)')
plt.legend()
plt.grid(True)
plt.show()

# ----------------------------------------------------
# Plot: Error (Residuals) Comparison
# ----------------------------------------------------
errors_vmd = y_test_single - y_pred_vmd
errors_adv = y_test_single - y_pred_adv

plt.figure(figsize=(10, 4))
plt.plot(errors_vmd, label='VMD + Attention-LSTM Errors', color='purple')
plt.plot(errors_adv, label='Advanced BiLSTM Errors', color='green', alpha=0.7)
plt.title('Prediction Error Comparison')
plt.xlabel('Time Steps')
plt.ylabel('Error')
plt.legend()
plt.grid(True)
plt.show()

